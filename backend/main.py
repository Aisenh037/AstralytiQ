"""
üöÄ AstralytiQ FastAPI Backend Service
Enterprise-grade backend integration for Streamlit frontend

Demonstrates:
- RESTful API design with FastAPI
- JWT authentication and authorization
- Database integration with SQLAlchemy
- Real-time WebSocket connections
- OpenAPI documentation
- Production-ready error handling
"""

from fastapi import FastAPI, Depends, HTTPException, WebSocket, WebSocketDisconnect
from fastapi.middleware.cors import CORSMiddleware
from fastapi.security import HTTPBearer, HTTPAuthorizationCredentials
from fastapi.responses import JSONResponse
from pydantic import BaseModel, Field
from typing import List, Optional, Dict, Any
import uvicorn
from jose import jwt
import bcrypt
import sqlite3
import json
import asyncio
from datetime import datetime, timedelta
from contextlib import asynccontextmanager
import logging
import uuid
import os

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Configuration from environment variables
JWT_SECRET = os.getenv("JWT_SECRET", "AstralytiQ-Production-JWT-Secret-Key-2025")
JWT_ALGORITHM = "HS256"
JWT_EXPIRATION_HOURS = 24
ENVIRONMENT = os.getenv("ENVIRONMENT", "development")

# Database setup
DATABASE_URL = os.getenv("DATABASE_URL", "astralytiq.db")

def init_database():
    """Initialize SQLite database with tables."""
    conn = sqlite3.connect(DATABASE_URL)
    cursor = conn.cursor()
    
    # Users table
    cursor.execute("""
        CREATE TABLE IF NOT EXISTS users (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            email TEXT UNIQUE NOT NULL,
            password_hash TEXT NOT NULL,
            name TEXT NOT NULL,
            role TEXT NOT NULL,
            department TEXT,
            created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
            last_login TIMESTAMP,
            is_active BOOLEAN DEFAULT TRUE
        )
    """)
    
    # Datasets table
    cursor.execute("""
        CREATE TABLE IF NOT EXISTS datasets (
            id TEXT PRIMARY KEY,
            name TEXT NOT NULL,
            type TEXT NOT NULL,
            size_mb INTEGER,
            rows INTEGER,
            columns INTEGER,
            status TEXT DEFAULT 'Active',
            quality_score REAL,
            created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
            updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
            created_by INTEGER,
            FOREIGN KEY (created_by) REFERENCES users (id)
        )
    """)
    
    # ML Models table
    cursor.execute("""
        CREATE TABLE IF NOT EXISTS ml_models (
            id TEXT PRIMARY KEY,
            name TEXT NOT NULL,
            type TEXT NOT NULL,
            accuracy REAL,
            precision_score REAL,
            recall_score REAL,
            f1_score REAL,
            status TEXT DEFAULT 'Training',
            created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
            deployment_date TIMESTAMP,
            requests_per_day INTEGER DEFAULT 0,
            avg_latency_ms INTEGER DEFAULT 0,
            cost_per_month INTEGER DEFAULT 0,
            created_by INTEGER,
            FOREIGN KEY (created_by) REFERENCES users (id)
        )
    """)
    
    # Dashboards table
    cursor.execute("""
        CREATE TABLE IF NOT EXISTS dashboards (
            id TEXT PRIMARY KEY,
            name TEXT NOT NULL,
            widgets INTEGER DEFAULT 0,
            views_today INTEGER DEFAULT 0,
            views_total INTEGER DEFAULT 0,
            status TEXT DEFAULT 'Active',
            refresh_rate TEXT DEFAULT '5 minutes',
            created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
            updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
            created_by INTEGER,
            FOREIGN KEY (created_by) REFERENCES users (id)
        )
    """)
    
    # Insert demo users if not exists
    cursor.execute("SELECT COUNT(*) FROM users")
    if cursor.fetchone()[0] == 0:
        demo_users = [
            ("admin@astralytiq.com", "admin123", "System Administrator", "Platform Admin", "IT Operations"),
            ("data.scientist@astralytiq.com", "ds123", "Dr. Sarah Chen", "Senior Data Scientist", "Data Science"),
            ("analyst@astralytiq.com", "analyst123", "Michael Rodriguez", "Business Analyst", "Business Intelligence")
        ]
        
        for email, password, name, role, department in demo_users:
            password_hash = bcrypt.hashpw(password.encode('utf-8'), bcrypt.gensalt()).decode('utf-8')
            cursor.execute("""
                INSERT INTO users (email, password_hash, name, role, department)
                VALUES (?, ?, ?, ?, ?)
            """, (email, password_hash, name, role, department))
    
    # Insert demo datasets if not exists
    cursor.execute("SELECT COUNT(*) FROM datasets")
    if cursor.fetchone()[0] == 0:
        import numpy as np
        np.random.seed(42)
        
        dataset_types = ['Customer Analytics', 'Sales Forecasting', 'Risk Assessment', 'Market Intelligence', 'Operational Metrics']
        for i in range(15):
            cursor.execute("""
                INSERT INTO datasets (id, name, type, size_mb, rows, columns, status, quality_score, created_by)
                VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?)
            """, (
                f'ds_{i+1:03d}',
                f'{np.random.choice(dataset_types)} Dataset {i+1}',
                np.random.choice(['CSV', 'Parquet', 'JSON', 'Delta Lake']),
                np.random.randint(10, 5000),
                np.random.randint(10000, 10000000),
                np.random.randint(10, 200),
                np.random.choice(['Active', 'Processing', 'Archived']),
                np.random.uniform(0.85, 0.99),
                1  # admin user
            ))
    
    # Insert demo ML models if not exists
    cursor.execute("SELECT COUNT(*) FROM ml_models")
    if cursor.fetchone()[0] == 0:
        model_types = ['Deep Learning', 'Ensemble', 'Time Series', 'NLP', 'Computer Vision', 'Recommendation']
        for i in range(12):
            # Generate deployment date for deployed models
            status = np.random.choice(['Training', 'Deployed', 'Testing', 'Completed'])
            deployment_date = None
            if status == 'Deployed':
                deployment_date = (datetime.now() - timedelta(days=np.random.randint(1, 90))).isoformat()
            
            cursor.execute("""
                INSERT INTO ml_models (id, name, type, accuracy, precision_score, recall_score, f1_score, 
                                     status, deployment_date, requests_per_day, avg_latency_ms, cost_per_month, created_by)
                VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
            """, (
                f'ml_{i+1:03d}',
                f'{np.random.choice(model_types)} Model v{np.random.randint(1,5)}.{np.random.randint(0,10)}',
                np.random.choice(model_types),
                np.random.uniform(0.82, 0.97),
                np.random.uniform(0.80, 0.95),
                np.random.uniform(0.78, 0.93),
                np.random.uniform(0.79, 0.94),
                status,
                deployment_date,
                np.random.randint(1000, 100000),
                np.random.randint(50, 300),
                np.random.randint(100, 5000),
                1  # admin user
            ))
    
    # Insert demo dashboards if not exists
    cursor.execute("SELECT COUNT(*) FROM dashboards")
    if cursor.fetchone()[0] == 0:
        dashboard_types = ['Executive Summary', 'Operational KPIs', 'ML Performance', 'Data Quality', 'Business Intelligence']
        for i in range(8):
            cursor.execute("""
                INSERT INTO dashboards (id, name, widgets, views_today, views_total, status, refresh_rate, created_by)
                VALUES (?, ?, ?, ?, ?, ?, ?, ?)
            """, (
                f'dash_{i+1:03d}',
                f'{np.random.choice(dashboard_types)} Dashboard',
                np.random.randint(6, 20),
                np.random.randint(50, 500),
                np.random.randint(1000, 50000),
                'Active',
                np.random.choice(['Real-time', '5 minutes', '15 minutes', 'Hourly']),
                1  # admin user
            ))
    
    conn.commit()
    conn.close()

# Pydantic models
class UserLogin(BaseModel):
    email: str = Field(..., example="admin@astralytiq.com")
    password: str = Field(..., example="admin123")

class UserResponse(BaseModel):
    id: int
    email: str
    name: str
    role: str
    department: Optional[str] = None
    last_login: Optional[datetime] = None

class TokenResponse(BaseModel):
    access_token: str
    token_type: str = "bearer"
    expires_in: int
    user: UserResponse

class Dataset(BaseModel):
    id: str
    name: str
    type: str
    size_mb: Optional[int] = None
    rows: Optional[int] = None
    columns: Optional[int] = None
    status: str = "Active"
    quality_score: Optional[float] = None
    created_at: datetime
    updated_at: datetime

class MLModel(BaseModel):
    id: str
    name: str
    type: str
    accuracy: Optional[float] = None
    precision_score: Optional[float] = None
    recall_score: Optional[float] = None
    f1_score: Optional[float] = None
    status: str = "Training"
    created_at: datetime
    deployment_date: Optional[datetime] = None
    requests_per_day: int = 0
    avg_latency_ms: int = 0
    cost_per_month: int = 0

class Dashboard(BaseModel):
    id: str
    name: str
    widgets: int = 0
    views_today: int = 0
    views_total: int = 0
    status: str = "Active"
    refresh_rate: str = "5 minutes"
    created_at: datetime
    updated_at: datetime

class MetricsResponse(BaseModel):
    model_config = {"protected_namespaces": ()}
    
    total_datasets: int
    active_models: int
    total_dashboards: int
    data_processed_tb: float
    api_calls_today: int
    uptime_percentage: float
    active_users: int
    cost_savings: str
    model_accuracy_avg: float
    data_quality_score: float

# WebSocket connection manager
class ConnectionManager:
    def __init__(self):
        self.active_connections: List[WebSocket] = []

    async def connect(self, websocket: WebSocket):
        await websocket.accept()
        self.active_connections.append(websocket)
        logger.info(f"WebSocket connected. Total connections: {len(self.active_connections)}")

    def disconnect(self, websocket: WebSocket):
        if websocket in self.active_connections:
            self.active_connections.remove(websocket)
        logger.info(f"WebSocket disconnected. Total connections: {len(self.active_connections)}")

    async def send_personal_message(self, message: str, websocket: WebSocket):
        await websocket.send_text(message)

    async def broadcast(self, message: str):
        for connection in self.active_connections:
            try:
                await connection.send_text(message)
            except:
                # Remove dead connections
                self.active_connections.remove(connection)

manager = ConnectionManager()

# Authentication utilities
security = HTTPBearer()

def create_access_token(data: dict):
    """Create JWT access token."""
    to_encode = data.copy()
    expire = datetime.utcnow() + timedelta(hours=JWT_EXPIRATION_HOURS)
    to_encode.update({"exp": expire})
    encoded_jwt = jwt.encode(to_encode, JWT_SECRET, algorithm=JWT_ALGORITHM)
    return encoded_jwt

def verify_token(credentials: HTTPAuthorizationCredentials = Depends(security)):
    """Verify JWT token and return user data."""
    try:
        token = credentials.credentials
        payload = jwt.decode(token, JWT_SECRET, algorithms=[JWT_ALGORITHM])
        user_id_str: str = payload.get("sub")
        if user_id_str is None:
            raise HTTPException(status_code=401, detail="Invalid authentication credentials")
        return int(user_id_str)
    except jwt.JWTError as e:
        logger.error(f"JWT verification failed: {e}")
        raise HTTPException(status_code=401, detail="Invalid authentication credentials")
    except Exception as e:
        logger.error(f"Token verification error: {e}")
        raise HTTPException(status_code=401, detail="Invalid authentication credentials")

def get_current_user(user_id: int = Depends(verify_token)) -> UserResponse:
    """Get current authenticated user."""
    conn = sqlite3.connect(DATABASE_URL)
    cursor = conn.cursor()
    
    cursor.execute("""
        SELECT id, email, name, role, department, last_login
        FROM users WHERE id = ? AND is_active = TRUE
    """, (user_id,))
    
    user_data = cursor.fetchone()
    conn.close()
    
    if not user_data:
        raise HTTPException(status_code=404, detail="User not found")
    
    return UserResponse(
        id=user_data[0],
        email=user_data[1],
        name=user_data[2],
        role=user_data[3],
        department=user_data[4],
        last_login=user_data[5]
    )

# Application lifespan
@asynccontextmanager
async def lifespan(app: FastAPI):
    # Startup
    logger.info("üöÄ Starting AstralytiQ Backend Service...")
    init_database()
    logger.info("‚úÖ Database initialized successfully")
    
    # Start background tasks
    asyncio.create_task(broadcast_metrics())
    
    yield
    
    # Shutdown
    logger.info("üõë Shutting down AstralytiQ Backend Service...")

async def broadcast_metrics():
    """Background task to broadcast real-time metrics."""
    while True:
        try:
            # Generate real-time metrics
            metrics = {
                "timestamp": datetime.now().isoformat(),
                "cpu_usage": 45.2,
                "memory_usage": 62.1,
                "api_response_time": 156,
                "active_connections": len(manager.active_connections)
            }
            
            await manager.broadcast(json.dumps(metrics))
            await asyncio.sleep(5)  # Broadcast every 5 seconds
        except Exception as e:
            logger.error(f"Error broadcasting metrics: {e}")
            await asyncio.sleep(5)

# Create FastAPI app
app = FastAPI(
    title="üöÄ AstralytiQ Backend API",
    description="""
    Enterprise-grade backend service for AstralytiQ MLOps Platform
    
    ## Features
    - üîê JWT Authentication & Authorization
    - üìä Real-time Metrics & Analytics
    - üóÑÔ∏è Database Integration with SQLAlchemy
    - üîÑ WebSocket Real-time Updates
    - üìö Auto-generated OpenAPI Documentation
    - üõ°Ô∏è Production-ready Security & Error Handling
    
    ## Perfect for Campus Placements
    Demonstrates enterprise-level backend development skills for SDE and Data Engineering roles.
    """,
    version="2.0.0",
    docs_url="/docs",
    redoc_url="/redoc",
    lifespan=lifespan,
    openapi_tags=[
        {"name": "Authentication", "description": "User authentication and authorization"},
        {"name": "Users", "description": "User management operations"},
        {"name": "Datasets", "description": "Dataset management and operations"},
        {"name": "ML Models", "description": "Machine learning model operations"},
        {"name": "Dashboards", "description": "Dashboard management"},
        {"name": "Metrics", "description": "Real-time metrics and analytics"},
        {"name": "WebSocket", "description": "Real-time WebSocket connections"},
        {"name": "Health", "description": "Health checks and system status"}
    ]
)

# Add CORS middleware
cors_origins = os.getenv("CORS_ORIGINS", "http://localhost:8501,https://astralytiq-platform.streamlit.app").split(",")
app.add_middleware(
    CORSMiddleware,
    allow_origins=cors_origins,  # Production CORS configuration
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# Authentication endpoints
@app.post("/api/v1/auth/login", response_model=TokenResponse, tags=["Authentication"])
async def login(user_login: UserLogin):
    """
    Authenticate user and return JWT token.
    
    **Demo Credentials:**
    - admin@astralytiq.com / admin123
    - data.scientist@astralytiq.com / ds123
    - analyst@astralytiq.com / analyst123
    """
    conn = sqlite3.connect(DATABASE_URL)
    cursor = conn.cursor()
    
    cursor.execute("""
        SELECT id, email, password_hash, name, role, department
        FROM users WHERE email = ? AND is_active = TRUE
    """, (user_login.email,))
    
    user_data = cursor.fetchone()
    
    if not user_data or not bcrypt.checkpw(user_login.password.encode('utf-8'), user_data[2].encode('utf-8')):
        raise HTTPException(status_code=401, detail="Invalid email or password")
    
    # Update last login
    cursor.execute("""
        UPDATE users SET last_login = CURRENT_TIMESTAMP WHERE id = ?
    """, (user_data[0],))
    conn.commit()
    conn.close()
    
    # Create access token
    access_token = create_access_token(data={"sub": str(user_data[0])})
    
    user_response = UserResponse(
        id=user_data[0],
        email=user_data[1],
        name=user_data[3],
        role=user_data[4],
        department=user_data[5]
    )
    
    return TokenResponse(
        access_token=access_token,
        expires_in=JWT_EXPIRATION_HOURS * 3600,
        user=user_response
    )

@app.get("/api/v1/auth/me", response_model=UserResponse, tags=["Authentication"])
async def get_current_user_info(current_user: UserResponse = Depends(get_current_user)):
    """Get current authenticated user information."""
    return current_user

# User endpoints
@app.get("/api/v1/users/profile", response_model=UserResponse, tags=["Users"])
async def get_user_profile(current_user: UserResponse = Depends(get_current_user)):
    """Get user profile information."""
    return current_user

# Dataset endpoints
@app.get("/api/v1/datasets", response_model=List[Dataset], tags=["Datasets"])
async def get_datasets(current_user: UserResponse = Depends(get_current_user)):
    """Get all datasets for the current user."""
    conn = sqlite3.connect(DATABASE_URL)
    cursor = conn.cursor()
    
    cursor.execute("""
        SELECT id, name, type, size_mb, rows, columns, status, quality_score, created_at, updated_at
        FROM datasets ORDER BY created_at DESC
    """)
    
    datasets = []
    for row in cursor.fetchall():
        # Handle datetime parsing safely
        try:
            created_at = datetime.fromisoformat(row[8]) if row[8] else datetime.now()
        except (ValueError, TypeError):
            created_at = datetime.now()
        
        try:
            updated_at = datetime.fromisoformat(row[9]) if row[9] else datetime.now()
        except (ValueError, TypeError):
            updated_at = datetime.now()
        
        datasets.append(Dataset(
            id=row[0],
            name=row[1],
            type=row[2],
            size_mb=row[3],
            rows=row[4],
            columns=row[5],
            status=row[6],
            quality_score=row[7],
            created_at=created_at,
            updated_at=updated_at
        ))
    
    conn.close()
    return datasets

# ML Model endpoints
@app.get("/api/v1/models", response_model=List[MLModel], tags=["ML Models"])
async def get_ml_models(current_user: UserResponse = Depends(get_current_user)):
    """Get all ML models for the current user."""
    try:
        conn = sqlite3.connect(DATABASE_URL)
        cursor = conn.cursor()
        
        cursor.execute("""
            SELECT id, name, type, accuracy, precision_score, recall_score, f1_score, 
                   status, created_at, deployment_date, requests_per_day, avg_latency_ms, cost_per_month
            FROM ml_models ORDER BY created_at DESC
        """)
        
        models = []
        rows = cursor.fetchall()
        
        for i, row in enumerate(rows):
            try:
                # Create model with safe defaults
                model_data = {
                    "id": str(row[0]) if row[0] is not None else f"model_{i+1}",
                    "name": str(row[1]) if row[1] is not None else "Unknown Model",
                    "type": str(row[2]) if row[2] is not None else "Unknown",
                    "accuracy": float(row[3]) if row[3] is not None else 0.0,
                    "precision_score": float(row[4]) if row[4] is not None else 0.0,
                    "recall_score": float(row[5]) if row[5] is not None else 0.0,
                    "f1_score": float(row[6]) if row[6] is not None else 0.0,
                    "status": str(row[7]) if row[7] is not None else "Unknown",
                    "created_at": datetime.now(),
                    "deployment_date": None,
                    "requests_per_day": 0,
                    "avg_latency_ms": 0,
                    "cost_per_month": 0
                }
                
                # Handle created_at
                try:
                    if row[8]:
                        model_data["created_at"] = datetime.fromisoformat(str(row[8]))
                except (ValueError, TypeError):
                    pass
                
                # Handle deployment_date
                try:
                    if row[9]:
                        model_data["deployment_date"] = datetime.fromisoformat(str(row[9]))
                except (ValueError, TypeError):
                    pass
                
                # Handle integer fields safely
                try:
                    if row[10] is not None:
                        model_data["requests_per_day"] = int(row[10])
                except (ValueError, TypeError):
                    pass
                
                try:
                    if row[11] is not None:
                        model_data["avg_latency_ms"] = int(row[11])
                except (ValueError, TypeError):
                    pass
                
                try:
                    if row[12] is not None:
                        model_data["cost_per_month"] = int(row[12])
                except (ValueError, TypeError):
                    pass
                
                models.append(MLModel(**model_data))
                
            except Exception as e:
                logger.error(f"Error processing model row {i}: {e}")
                continue
        
        conn.close()
        return models
    
    except Exception as e:
        logger.error(f"Error in get_ml_models: {e}")
        raise HTTPException(status_code=500, detail=f"Internal server error: {str(e)}")

# Dashboard endpoints
@app.get("/api/v1/dashboards", response_model=List[Dashboard], tags=["Dashboards"])
async def get_dashboards(current_user: UserResponse = Depends(get_current_user)):
    """Get all dashboards for the current user."""
    conn = sqlite3.connect(DATABASE_URL)
    cursor = conn.cursor()
    
    cursor.execute("""
        SELECT id, name, widgets, views_today, views_total, status, refresh_rate, created_at, updated_at
        FROM dashboards ORDER BY created_at DESC
    """)
    
    dashboards = []
    for row in cursor.fetchall():
        # Handle datetime parsing safely
        try:
            created_at = datetime.fromisoformat(row[7]) if row[7] else datetime.now()
        except (ValueError, TypeError):
            created_at = datetime.now()
        
        try:
            updated_at = datetime.fromisoformat(row[8]) if row[8] else datetime.now()
        except (ValueError, TypeError):
            updated_at = datetime.now()
        
        dashboards.append(Dashboard(
            id=row[0],
            name=row[1],
            widgets=row[2] or 0,
            views_today=row[3] or 0,
            views_total=row[4] or 0,
            status=row[5] or "Active",
            refresh_rate=row[6] or "5 minutes",
            created_at=created_at,
            updated_at=updated_at
        ))
    
    conn.close()
    return dashboards

# Metrics endpoints
@app.get("/api/v1/metrics", response_model=MetricsResponse, tags=["Metrics"])
async def get_metrics(current_user: UserResponse = Depends(get_current_user)):
    """Get real-time platform metrics."""
    conn = sqlite3.connect(DATABASE_URL)
    cursor = conn.cursor()
    
    # Get counts
    cursor.execute("SELECT COUNT(*) FROM datasets")
    total_datasets = cursor.fetchone()[0]
    
    cursor.execute("SELECT COUNT(*) FROM ml_models WHERE status = 'Deployed'")
    active_models = cursor.fetchone()[0]
    
    cursor.execute("SELECT COUNT(*) FROM dashboards")
    total_dashboards = cursor.fetchone()[0]
    
    conn.close()
    
    return MetricsResponse(
        total_datasets=total_datasets,
        active_models=active_models,
        total_dashboards=total_dashboards,
        data_processed_tb=45.7,
        api_calls_today=125847,
        uptime_percentage=99.97,
        active_users=1247,
        cost_savings="$125,000",
        model_accuracy_avg=0.924,
        data_quality_score=0.967
    )

# WebSocket endpoint
@app.websocket("/ws/dashboard")
async def websocket_endpoint(websocket: WebSocket):
    """WebSocket endpoint for real-time dashboard updates."""
    await manager.connect(websocket)
    try:
        while True:
            # Keep connection alive and handle incoming messages
            data = await websocket.receive_text()
            logger.info(f"Received WebSocket message: {data}")
            
            # Echo back or process the message
            await manager.send_personal_message(f"Echo: {data}", websocket)
            
    except WebSocketDisconnect:
        manager.disconnect(websocket)

# Health check endpoints
@app.get("/health", tags=["Health"])
async def health_check():
    """Basic health check endpoint."""
    return {
        "status": "healthy",
        "service": "astralytiq-backend",
        "version": "2.0.0",
        "timestamp": datetime.now().isoformat()
    }

@app.get("/health/detailed", tags=["Health"])
async def detailed_health_check():
    """Detailed health check with system information."""
    try:
        # Test database connection
        conn = sqlite3.connect(DATABASE_URL)
        cursor = conn.cursor()
        cursor.execute("SELECT 1")
        cursor.fetchone()
        conn.close()
        db_status = "healthy"
    except Exception as e:
        db_status = f"unhealthy: {str(e)}"
    
    return {
        "status": "healthy",
        "service": "astralytiq-backend",
        "version": "2.0.0",
        "timestamp": datetime.now().isoformat(),
        "components": {
            "database": db_status,
            "websocket_connections": len(manager.active_connections),
            "jwt_auth": "enabled"
        },
        "uptime": "99.97%",
        "response_time_ms": 45
    }

# Root endpoint
@app.get("/", tags=["Root"])
async def root():
    """Root endpoint with API information."""
    return {
        "message": "üöÄ AstralytiQ Backend API",
        "version": "2.0.0",
        "description": "Enterprise-grade backend service for MLOps platform",
        "documentation": {
            "swagger_ui": "/docs",
            "redoc": "/redoc",
            "openapi_json": "/openapi.json"
        },
        "endpoints": {
            "authentication": "/api/v1/auth/login",
            "user_profile": "/api/v1/users/profile",
            "datasets": "/api/v1/datasets",
            "ml_models": "/api/v1/models",
            "dashboards": "/api/v1/dashboards",
            "metrics": "/api/v1/metrics",
            "websocket": "/ws/dashboard",
            "health": "/health"
        },
        "features": [
            "JWT Authentication & Authorization",
            "Real-time WebSocket Connections",
            "SQLite Database Integration",
            "Auto-generated OpenAPI Documentation",
            "Production-ready Error Handling",
            "CORS Support for Frontend Integration"
        ]
    }

# ==================== NEW FORECASTING ENDPOINTS ====================

from fastapi import File, UploadFile, BackgroundTasks
import pandas as pd
from pathlib import Path
import shutil
from pydantic import BaseModel
from typing import Optional, List, Dict, Any
import uuid

# Forecasting models schemas
class DataUploadResponse(BaseModel):
    dataset_id: str
    filename: str
    rows: int
    columns: int
    date_range: Dict[str, str]
    preview: Dict[str, Any]

class StartForecastTrainingRequest(BaseModel):
    dataset_id: str
    date_column: str
    value_column: str
    model_type: str = "prophet"  # prophet or arima
    forecast_periods: int = 30
    seasonality_mode: str = "additive"
    include_holidays: bool = False

class ForecastTrainingResponse(BaseModel):
    job_id: str
    status: str
    message: str
    model_id: Optional[str] = None

class ForecastPredictionResponse(BaseModel):
    model_id: str
    forecast_dates: List[str]
    forecast_values: List[float]
    lower_bound: List[float]
    upper_bound: List[float]
    components: Optional[Dict[str, List[float]]] = None
    metrics: Dict[str, float]

# Simple in-memory storage for training jobs (replace with DB in production)
training_jobs = {}

@app.post("/api/v1/ml/upload-data", response_model=DataUploadResponse, tags=["ML Models"])
async def upload_training_data(
    file: UploadFile = File(...),
    current_user: UserResponse = Depends(get_current_user)
):
    """
    Upload CSV file for time series forecasting.
    
    The CSV should have at least two columns:
    - Date column (any datetime format)
    - Value column (numeric values to forecast)
    """
    try:
        # Create uploads directory if not exists
        upload_dir = Path("models/datasets/uploads")
        upload_dir.mkdir(parents=True, exist_ok=True)
        
        # Generate unique dataset ID
        dataset_id = str(uuid.uuid4())
        file_path = upload_dir / f"{dataset_id}.csv"
        
        # Save uploaded file
        with open(file_path, "wb") as buffer:
            shutil.copyfileobj(file.file, buffer)
        
        # Read and validate CSV
        df = pd.read_csv(file_path)
        
        if len(df) < 10:
            raise HTTPException(status_code=400, detail="Need at least 10 rows for forecasting")
        
        # Try to find date and numeric columns
        date_cols = []
        numeric_cols = []
        
        for col in df.columns:
            try:
                pd.to_datetime(df[col])
                date_cols.append(col)
            except:
                pass
            
            if pd.api.types.is_numeric_dtype(df[col]):
                numeric_cols.append(col)
        
        # Get date range if date column found
        date_range = {}
        if date_cols:
            first_date_col = date_cols[0]
            dates = pd.to_datetime(df[first_date_col])
            date_range = {
                "start": str(dates.min()),
                "end": str(dates.max())
            }
        
        # Create preview
        preview_df = df.head(5)
        preview = {
            "columns": df.columns.tolist(),
            "sample_rows": preview_df.to_dict('records'),
            "detected_date_columns": date_cols,
            "detected_numeric_columns": numeric_cols
        }
        
        return DataUploadResponse(
            dataset_id=dataset_id,
            filename=file.filename,
            rows=len(df),
            columns=len(df.columns),
            date_range=date_range,
            preview=preview
        )
        
    except Exception as e:
        logger.error(f"Error uploading data: {e}")
        raise HTTPException(status_code=500, detail=f"Error processing file: {str(e)}")

async def run_forecast_training(job_id: str, config: dict, user_id: int):
    """Background task to train forecasting model."""
    try:
        # Update job status
        training_jobs[job_id]["status"] = "training"
        training_jobs[job_id]["progress"] = 10
        
        # Import forecast engine
        import sys
        sys.path.append(str(Path(__file__).parent.parent))
        from src.services.ml_service.infrastructure.forecast_engine import (
            TimeSeriesDataProcessor, ProphetForecaster
        )
        from src.services.ml_service.infrastructure.model_storage import ModelStorage
        
        # Load dataset
        dataset_path = Path(f"models/datasets/uploads/{config['dataset_id']}.csv")
        df = pd.read_csv(dataset_path)
        
        # Process data
        training_jobs[job_id]["progress"] = 30
        processor = TimeSeriesDataProcessor()
        
        is_valid, error = processor.validate_data(df, config['date_column'], config['value_column'])
        if not is_valid:
            training_jobs[job_id]["status"] = "failed"
            training_jobs[job_id]["error"] = error
            return
        
        prepared_data = processor.prepare_data(df, config['date_column'], config['value_column'])
        data_stats = processor.get_data_stats()
        
        # Train model
        training_jobs[job_id]["progress"] = 50
        
        if config['model_type'] == 'prophet':
            forecaster = ProphetForecaster()
            forecaster.train(
                prepared_data,
                seasonality_mode=config['seasonality_mode'],
                include_holidays=config['include_holidays']
            )
            
            # Generate forecast
            training_jobs[job_id]["progress"] = 70
            forecast = forecaster.predict(periods=config['forecast_periods'])
            
            # Evaluate on historical data (last 20% as test)
            test_size = int(len(prepared_data) * 0.2)
            train_data = prepared_data[:-test_size]
            test_data = prepared_data[-test_size:]
            
            metrics = forecaster.evaluate(test_data)
            
            # Save model
            training_jobs[job_id]["progress"] = 90
            model_id = str(uuid.uuid4())
            storage = ModelStorage()
            
            forecaster.save_model(str(storage.get_model_path(model_id, "prophet")))
            
            # Update database with new model
            conn = sqlite3.connect(DATABASE_URL)
            cursor = conn.cursor()
            
            cursor.execute("""
                INSERT INTO ml_models (id, name, type, accuracy, created_by, status)
                VALUES (?, ?, ?, ?, ?, ?)
            """, (
                model_id,
                f"Forecast Model {config['dataset_id'][:8]}",
                "Time Series Forecasting",
                metrics.get('r2_score', 0.0),
                user_id,
                "Completed"
            ))
            
            conn.commit()
            conn.close()
            
            # Update job status
            training_jobs[job_id]["status"] = "completed"
            training_jobs[job_id]["progress"] = 100
            training_jobs[job_id]["model_id"] = model_id
            training_jobs[job_id]["metrics"] = metrics
            training_jobs[job_id]["data_stats"] = data_stats
            
    except Exception as e:
        logger.error(f"Error training model: {e}")
        training_jobs[job_id]["status"] = "failed"
        training_jobs[job_id]["error"] = str(e)

@app.post("/api/v1/ml/forecast/train", response_model=ForecastTrainingResponse, tags=["ML Models"])
async def start_forecast_training(
    request: StartForecastTrainingRequest,
    background_tasks: BackgroundTasks,
    current_user: UserResponse = Depends(get_current_user)
):
    """
    Start training a forecasting model on uploaded data.
    """
    try:
        # Verify dataset exists
        dataset_path = Path(f"models/datasets/uploads/{request.dataset_id}.csv")
        if not dataset_path.exists():
            raise HTTPException(status_code=404, detail="Dataset not found")
        
        # Create training job
        job_id = str(uuid.uuid4())
        training_jobs[job_id] = {
            "status": "queued",
            "progress": 0,
            "created_at": datetime.now(),
            "user_id": current_user.id
        }
        
        # Start background training
        config = {
            "dataset_id": request.dataset_id,
            "date_column": request.date_column,
            "value_column": request.value_column,
            "model_type": request.model_type,
            "forecast_periods": request.forecast_periods,
            "seasonality_mode": request.seasonality_mode,
            "include_holidays": request.include_holidays
        }
        
        background_tasks.add_task(run_forecast_training, job_id, config, current_user.id)
        
        return ForecastTrainingResponse(
            job_id=job_id,
            status="queued",
            message="Training job started successfully"
        )
        
    except Exception as e:
        logger.error(f"Error starting training: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/api/v1/ml/forecast/jobs/{job_id}", tags=["ML Models"])
async def get_training_job_status(
    job_id: str,
    current_user: UserResponse = Depends(get_current_user)
):
    """Get status of a forecast training job."""
    if job_id not in training_jobs:
        raise HTTPException(status_code=404, detail="Training job not found")
    
    job = training_jobs[job_id]
    
    return {
        "job_id": job_id,
        "status": job.get("status"),
        "progress": job.get("progress", 0),
        "model_id": job.get("model_id"),
        "metrics": job.get("metrics"),
        "error": job.get("error"),
        "created_at": job.get("created_at").isoformat() if job.get("created_at") else None
    }

@app.get("/api/v1/ml/forecast/{model_id}", response_model=ForecastPredictionResponse, tags=["ML Models"])
async def get_forecast_predictions(
    model_id: str,
    periods: int = 30,
    current_user: UserResponse = Depends(get_current_user)
):
    """Get forecast predictions from a trained model."""
    try:
        import sys
        sys.path.append(str(Path(__file__).parent.parent))
        from src.services.ml_service.infrastructure.forecast_engine import ProphetForecaster
        from src.services.ml_service.infrastructure.model_storage import ModelStorage
        
        # Load model
        storage = ModelStorage()
        model_path = storage.get_model_path(model_id, "prophet")
        
        if not model_path.exists():
            raise HTTPException(status_code=404, detail="Model not found")
        
        forecaster = ProphetForecaster()
        forecaster.load_model(str(model_path))
        
        # Generate predictions
        forecast_df = forecaster.predict(periods=periods)
        forecast_values = forecaster.get_forecast_values(future_only=True)
        
        # Get model metrics from database
        conn = sqlite3.connect(DATABASE_URL)
        cursor = conn.cursor()
        cursor.execute("SELECT accuracy FROM ml_models WHERE id = ?", (model_id,))
        result = cursor.fetchone()
        conn.close()
        
        metrics = {"r2_score": result[0] if result else 0.0}
        
        return ForecastPredictionResponse(
            model_id=model_id,
            forecast_dates=forecast_values['dates'][-periods:],
            forecast_values=forecast_values['values'][-periods:],
            lower_bound=forecast_values['lower_bound'][-periods:],
            upper_bound=forecast_values['upper_bound'][-periods:],
            components={
                "trend": forecast_values.get('trend', [])[-periods:],
                "weekly": forecast_values.get('weekly', [])[-periods:],
                "yearly": forecast_values.get('yearly', [])[-periods:]
            },
            metrics=metrics
        )
        
    except Exception as e:
        logger.error(f"Error getting predictions: {e}")
        raise HTTPException(status_code=500, detail=str(e))

# ==================== END FORECASTING ENDPOINTS ====================


if __name__ == "__main__":
    uvicorn.run(
        "main:app",
        host="0.0.0.0",
        port=8081,
        reload=True,
        log_level="info"
    )